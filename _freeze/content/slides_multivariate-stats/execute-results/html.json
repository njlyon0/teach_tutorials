{
  "hash": "744540c4fa6d41c519c76ece7d60c3d4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multivariate Statistics 101\"\nauthor: \"Nick J Lyon\"\nengine: knitr\nformat: \n  revealjs: \n    slide-number: c\n    scrollable: false\n    code-overflow: wrap\n    code-line-numbers: false\n    code-copy: hover\n    theme: [night, slides.scss]\n    reference-location: document\n---\n\n## Caveat Before We Begin {.smaller}\n\n::::{.columns}\n:::{.column width=\"60%\"}\n\n- Read this book {{< fa arrow-right >}}\n\n\\\n\n- Has a complete R appendix for:\n    - Every example\n    - Every figure\n    - Every operation\n\n\\\n\n- Essentially the book is written in R Markdown\n\n\\\n\n- Bonus: actually pretty engaging to read!\n    - Despite subject matter\n\n:::\n:::{.column width=\"40%\"}\n\n<img src=\"images/statistics/manly-navarro_multivar-primer-cover.jpg\" alt=\"Cover of the fourth edition of 'Multivariate Statistical Methods: A Primer' by Bryan FJ Manly and Jorge A Navarro Alberto\">\n\n:::\n::::\n\n## Tutorial Outline\n\n\\\n\n### [Background]{.purple}\n### [Resampling & Permutation]{.orange}\n### [Multivariate Data Visualization]{.gold}\n### [Principle Components Analysis]{.pink}\n### [Non-Metric Multidimensional Scaling]{.blue}\n\n## [Multivariate Background]{.purple} {.smaller}\n\n- **Multivariate data have more variables (_p_) than observations (_q_)**\n\n\\\n\n. . .\n\n- I.e., more columns than rows\n    - True of most ecology/evolution datasets\n\n\\\n\n. . .\n\n- Differs from univariate statistics\n    - Univariate explores variation in _one_ variable\n    - Multivariate explores variation in _many_ variables (plus potential inter-relationships)\n\n## [Resampling Methods]{.orange}\n\n- Frequentist statistics uses distributions _from theory_\n\n<p align=\"center\">\n<img src=\"images/statistics/distributions_theoretical.png\" alt=\"Graphs of several common data distributions found in theory\" width=\"80%\">\n</p>\n\n. . .\n\n- Resampling statistics uses distributions _from data_\n\n<p align=\"center\">\n<img src=\"images/statistics/distributions_from-data.png\" alt=\"Graph of a sort of irregular histogram\" width=\"15%\">\n</p>\n\n## [Theoretical Process]{.orange} \n\n1. Take samples from data (i.e., \"re-sample\")\n\n. . .\n\n2. Compare real observations to re-sampled groups\n\n. . .\n\n3. Evaluate significance\n\n## [Permutation Notes]{.orange} \n\n- Permutation methods are **non-parametric**\n    - Because they don't rely on a theoretical distribution\n\n\\\n\n. . .\n\n- Permutation methods are **flexible**\n    - Can assess standard & non-standard experimental designs\n    - Handle high-dimensional data (more variables than observations)\n\n## [Two Major \"Flavors\"]{.orange}{.smaller} \n\n::::{.columns}\n:::{.column width=\"50%\"}\n### Full Permutation\n\n- Permute whole dataset\n\n:::\n:::{.column width=\"50%\"}\n### Residual Permutation\n\n- Fit desired model\n- Permute the _residuals_\n    - Less sensitive to outliers\n:::\n::::\n\n<p align=\"center\">\n<img src=\"images/statistics/diagram_resid-perm-process.png\" alt=\"Diagram showing two X variables groups with a measured response (Y), model is fit and residuals are observed and compared to many different permutations (different group assignments) of the same residuals\" width=\"90%\">\n</p>\n\n## [Multivariate Visualization]{.gold} {.smaller} \n\n- Typically involves \"ordination\"\n\n\\\n\n. . . \n\n- Frequently uses \"multidimensional scaling\"\n    - I.e., getting from many variables to fewer, more easily visualizable variables\n    - Still representative of multivariate nature of data\n\n\\\n\n. . . \n\n- Common ordination methods include:\n    - <u>P</u>rincipal <u>C</u>omponents <u>A</u>nalysis (PCA)\n    - <u>P</u>rincipal **<u>Co</u>ordinates** <u>A</u>nalysis (PC**o**A)\n    - <u>N</u>onmetric <u>M</u>ultidimensional <u>S</u>caling (NMS)\n\n## [Principle Components Analysis]{.pink} {.smaller}\n\n- Goal: reduce number of variables\n\n\\\n\n. . . \n\n- Mechanism: create combinations of existing variables to summarize variation\n    - Want each combination to contain as much variation as possible\n    - Such that you approach 100% variation summarized in only a few combinations\n\n\\\n\n. . . \n\n- Result: number of principal components equal to number of observations\n    - Each principle component has a known % variation explained\n\n\n## [PCA Process]{.pink} {.smaller}\n\n- For variables (X~i~) you want to create indices (I~k~)\n\n\\\n\n. . .\n\n- Consider the following example:\n    - I~1~ = X~1~ + X~2~ + X~3~ + X~4~ + X~5~\n    - I~2~ = X~1~ [**-**]{.pink} X~2~ + X~3~ + X~4~ + X~5~\n    - I~3~ = X~1~ [**-**]{.pink} X~2~ [**-**]{.pink} X~3~ + X~4~ + X~5~\n    - ...\n    - I~k~ = X~1~ [**-**]{.pink} X~2~ [**-**]{.pink} X~3~ [**-**]{.pink} X~4~ [**-**]{.pink} X~5~\n\n\n## [PCA Special Consideration 1]{.pink} {.smaller}\n\n1. Axis orthogonality\n    - Axes are \"constrained to orthogonality\" because of goal of maximized explained variation\n    - Plain language: PC axes are perpendicular to one another\n\n\\\n\n. . .\n\n- Means PC~3~ through PC~_n_~ are defined _as soon as PC~1~ and PC~2~ are_\n    - Focusing on early PCs reduces the relevance of this issue\n\n## [PCA Special Consideration 2]{.pink} {.smaller}\n\n2. Not a hypothesis test\n\n\\\n\n. . .\n\n- PCA is great for visualizing patterns in data\n    - Not good for statistical evaluation\n\n\\\n\n. . .\n\n- I.e., PCA cannot--by itself--show support for your hypothesis\n\n\n## [Nonmetric Multidimensional Scaling]{.blue} {.smaller}\n\n- Goal: reduce number of variables\n    - Same as PCA!\n\n\\\n\n. . . \n\n- Mechanism: scale dissimilarity of points to minimize \"stress\"\n    - \"dissimilarity\" != \"distance\"\n    - Stress is a metric for tension between true spatial configuration of points versus the arrangement of their dissimilarity\n\n\\\n\n. . . \n\n- Result: number of NMS axes is defined by the user\n    - NMS reports stress of \"best\" solution (essentially a goodness of fit metric)\n\n## [NMS Process]{.blue} {.smaller}\n\n1. Choose a starting configuration of points (randomly)\n\n\\\n\n2. Move points around and measure stress at each configuration\n\n\\\n\n3. Repeat until stress has been minimized\n\n\\\n\n4. Return to step 1 with different starting points\n    - Necessary to avoid local stress minima\n\n\\\n\n5. Continue 1-4 until confident true minimum stress configuration has been found\n\n## [NMS Helicopter Analogy]{.blue} \n\n:::{.r-stack}\n\n![](images/statistics/nms_heli-01.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-02.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-03.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-04.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-05.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-06.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-07.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-08.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-09.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-10.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-11.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-12.jpg){.absolute top=150 left=25 .fragment .fade-in}\n![](images/statistics/nms_heli-13.jpg){.absolute top=150 left=25 .fragment .fade-in}\n\n:::\n\n# Multivariate Code Demo \n\n## Prepare {.smaller}\n\n- First, you'll need to install and load a few {{< fa brands r-project >}} R packages\n    - While not technically necessary, the `librarian` package makes library management _much_ simpler\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install librarian (if you need to)\n# install.packages(\"librarian\")\n\n# Install (if not already present) and load needed libraries\nlibrarian::shelf(vegan, RRPP, scatterplot3d, TeachingDemos, supportR)\n```\n:::\n\n\n## Lichen Data {.smaller}\n\n- The `vegan` package includes some lichen community composition data we can use for exploratory purposes\n\n\\\n\n- We'll begin by loading that and creating artificial groups for later analysis\n\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load vegan's lichen dataset\nutils::data(\"varespec\", package = 'vegan')\n\n# Make some columns of known number of groups\ntreatment <- c(rep.int(\"Trt1\", (nrow(varespec)/4)),\n               rep.int(\"Trt2\", (nrow(varespec)/4)),\n               rep.int(\"Trt3\", (nrow(varespec)/4)),\n               rep.int(\"Trt4\", (nrow(varespec)/4)))\n\n# And combine them into a single data object\nlichen_df <- cbind(treatment, varespec)\n```\n:::\n\n\n## Data Structure {.smaller}\n\n- This data object now has the following structure:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check lichen data structure\nstr(lichen_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t24 obs. of  45 variables:\n $ treatment: chr  \"Trt1\" \"Trt1\" \"Trt1\" \"Trt1\" ...\n $ Callvulg : num  0.55 0.67 0.1 0 0 ...\n $ Empenigr : num  11.13 0.17 1.55 15.13 12.68 ...\n $ Rhodtome : num  0 0 0 2.42 0 0 1.55 0 0.35 0.07 ...\n $ Vaccmyrt : num  0 0.35 0 5.92 0 ...\n $ Vaccviti : num  17.8 12.1 13.5 16 23.7 ...\n $ Pinusylv : num  0.07 0.12 0.25 0 0.03 0.12 0.1 0.1 0.05 0.12 ...\n $ Descflex : num  0 0 0 3.7 0 0.02 0.78 0 0.4 0 ...\n $ Betupube : num  0 0 0 0 0 0 0.02 0 0 0 ...\n $ Vacculig : num  1.6 0 0 1.12 0 0 2 0 0.2 0 ...\n $ Diphcomp : num  2.07 0 0 0 0 0 0 0 0 0.07 ...\n $ Dicrsp   : num  0 0.33 23.43 0 0 ...\n $ Dicrfusc : num  1.62 10.92 0 3.63 3.42 ...\n $ Dicrpoly : num  0 0.02 1.68 0 0.02 0.02 0 0.23 0.2 0 ...\n $ Hylosple : num  0 0 0 6.7 0 0 0 0 9.97 0 ...\n $ Pleuschr : num  4.67 37.75 32.92 58.07 19.42 ...\n $ Polypili : num  0.02 0.02 0 0 0.02 0.02 0 0 0 0 ...\n $ Polyjuni : num  0.13 0.23 0.23 0 2.12 1.58 0 0.02 0.08 0.02 ...\n $ Polycomm : num  0 0 0 0.13 0 0.18 0 0 0 0 ...\n $ Pohlnuta : num  0.13 0.03 0.32 0.02 0.17 0.07 0.1 0.13 0.07 0.03 ...\n $ Ptilcili : num  0.12 0.02 0.03 0.08 1.8 0.27 0.03 0.1 0.03 0.25 ...\n $ Barbhatc : num  0 0 0 0.08 0.02 0.02 0 0 0 0.07 ...\n $ Cladarbu : num  21.73 12.05 3.58 1.42 9.08 ...\n $ Cladrang : num  21.47 8.13 5.52 7.63 9.22 ...\n $ Cladstel : num  3.5 0.18 0.07 2.55 0.05 ...\n $ Cladunci : num  0.3 2.65 8.93 0.15 0.73 0.25 2.38 0.82 0.05 0.95 ...\n $ Cladcocc : num  0.18 0.13 0 0 0.08 0.1 0.17 0.15 0.02 0.17 ...\n $ Cladcorn : num  0.23 0.18 0.2 0.38 1.42 0.25 0.13 0.05 0.03 0.05 ...\n $ Cladgrac : num  0.25 0.23 0.48 0.12 0.5 0.18 0.18 0.22 0.07 0.23 ...\n $ Cladfimb : num  0.25 0.25 0 0.1 0.17 0.1 0.2 0.22 0.1 0.18 ...\n $ Cladcris : num  0.23 1.23 0.07 0.03 1.78 0.12 0.2 0.17 0.02 0.57 ...\n $ Cladchlo : num  0 0 0.1 0 0.05 0.05 0.02 0 0 0.02 ...\n $ Cladbotr : num  0 0 0.02 0.02 0.05 0.02 0 0 0.02 0.07 ...\n $ Cladamau : num  0.08 0 0 0 0 0 0 0 0 0 ...\n $ Cladsp   : num  0.02 0 0 0.02 0 0 0.02 0.02 0 0.07 ...\n $ Cetreric : num  0.02 0.15 0.78 0 0 0 0.02 0.18 0 0.18 ...\n $ Cetrisla : num  0 0.03 0.12 0 0 0 0 0.08 0.02 0.02 ...\n $ Flavniva : num  0.12 0 0 0 0.02 0.02 0 0 0 0 ...\n $ Nepharct : num  0.02 0 0 0 0 0 0 0 0 0 ...\n $ Stersp   : num  0.62 0.85 0.03 0 1.58 0.28 0 0.03 0.02 0.03 ...\n $ Peltapht : num  0.02 0 0 0.07 0.33 0 0 0 0 0.02 ...\n $ Icmaeric : num  0 0 0 0 0 0 0 0.07 0 0 ...\n $ Cladcerv : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Claddefo : num  0.25 1 0.33 0.15 1.97 0.37 0.15 0.67 0.08 0.47 ...\n $ Cladphyl : num  0 0 0 0 0 0 0 0 0 0 ...\n```\n\n\n:::\n:::\n\n\n- Each column is an abbreviated lichen species name and the values are % cover\n\n## [Permutation Analysis]{.orange} {.smaller}\n\n- First, we'll use the `RRPP` package to use permutation analysis\n    - H~A~: lichen community composition differs between at least two groups\n    - H~0~: lichen community composition _does not_ differ\n\n\\\n\n- Note that `RRPP` does require a special class of data object to perform analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make the special data object class required by RRPP\nlichen_rpdf <- RRPP::rrpp.data.frame(\"community\" = as.matrix(varespec),\n                                     \"treatment\" = lichen_df$treatment)\n\n# Fit permutation model\nlich_fit <- RRPP::lm.rrpp(community ~ treatment, data = lichen_rpdf, iter = 999, RRPP = T)\n```\n:::\n\n\n- Quick argument explanation:\n    - The `iter` argument is the number of permutations\n    - The `RRPP` _argument_ is whether to permute residuals (`TRUE`) or the full data (`FALSE`) \n\n## [Permutation Analysis Results]{.orange} {.smaller}\n\n- To check the results, we can use the `anova` function\n    - This also allows us to specify the desired effect type\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check out the results of the analysis!\nRRPP::anova.lm.rrpp(lich_fit, effect.type = \"F\", print.progress = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAnalysis of Variance, using Residual Randomization\nPermutation procedure: Randomization of null model residuals \nNumber of permutations: 1000 \nEstimation method: Ordinary Least Squares \nSums of Squares and Cross-products: Type I \nEffect sizes (Z) based on F distributions\n\n          Df    SS     MS     Rsq      F     Z Pr(>F)    \ntreatment  3 19602 6533.9 0.46682 5.8369 3.655  0.001 ***\nResiduals 20 22388 1119.4 0.53318                        \nTotal     23 41990                                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCall: RRPP::lm.rrpp(f1 = community ~ treatment, iter = 999, RRPP = T,  \n    data = lichen_rpdf)\n```\n\n\n:::\n:::\n\n\n- Now we get a fairly standard ANOVA table\n    - The `Z` column is the \"Z score\" and is essentially the effect size\n\n## [Permutation Pairwise Comparisons]{.orange} {.smaller}\n\n\n- Once we have our main results, we can also get pairwise comparison results\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform pairwise comparisons\nlich_pairs <- RRPP::pairwise(fit = lich_fit, groups = treatment)\n\n# Check results\nsummary(lich_pairs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nPairwise comparisons\n\nGroups: Trt1 Trt2 Trt3 Trt4 \n\nRRPP: 1000 permutations\n\nLS means:\nVectors hidden (use show.vectors = TRUE to view)\n\nPairwise distances between means, plus statistics\n                 d UCL (95%)          Z Pr > d\nTrt1:Trt2 16.48128  39.64863 -0.7289177  0.760\nTrt1:Trt3 38.04299  40.10757  1.5478150  0.067\nTrt1:Trt4 59.41369  38.99550  2.9255939  0.001\nTrt2:Trt3 37.15261  38.01919  1.5148939  0.063\nTrt2:Trt4 62.40360  39.28595  3.0093479  0.001\nTrt3:Trt4 50.44284  40.37863  2.2553908  0.005\n```\n\n\n:::\n:::\n\n\n- These show that the fourth treatment significantly differs from the other three\n    - And the third marginally differs from the first and second\n\n## [Multivariate Data Visualization]{.gold} {.smaller}\n\n- There are a few non-ordination ways of doing multivariate visualization\n    - Note that these are mostly for exploratory purposes\n\n\\\n\n- They're still helpful for checking the general 'vibe' of the data\n    - but they may not hold up to formal review processes\n\n## [3D Scatterplot]{.gold} {.smaller}\n\n- Perhaps the simplest mode of multivariate data visualization is just to make a 3D scatterplot!\n    - Still technically counts as \"multivariate\" visualization\n\n\\\n\n- Primary benefit is that interpretation is pretty straightforward\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make 3D scatterplot with `scatterplot3d` library\nscatterplot3d::scatterplot3d(lichen_df$Callvulg, lichen_df$Empenigr, lichen_df$Rhodtome,\n                     xlab = \"Callvulg\", ylab = \"Empenigr\", zlab = \"Rhodtome\")\n```\n\n::: {.cell-output-display}\n![](slides_multivariate-stats_files/figure-revealjs/viz-3d-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## [Chernoff Faces]{.gold} {.smaller}\n\n- Some people have tried to make human's capacity for comparing faces into a tool for data visualization\n    - Data are transformed into human(-ish) faces with different dimensions\n    - I find these _very_ scary\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make a matrix out of your desired data\nlich_mat <- data.matrix(varespec)\n\n# Generate Chernoff face graph\nTeachingDemos::faces2(lich_mat, labels = lichen_df$treatment, scale = \"center\")\n```\n\n::: {.cell-output-display}\n![](slides_multivariate-stats_files/figure-revealjs/viz-faces-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## [Star Plots]{.gold} {.smaller}\n\n- Perhaps most usefully, you can just make \"star plots\" to check multivariate data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create star plots\ngraphics::stars(x = varespec, labels = lichen_df$treatment, key.loc = c(16, 9))\n```\n\n::: {.cell-output-display}\n![](slides_multivariate-stats_files/figure-revealjs/viz-stars-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## [Principal Components]{.pink} {.smaller}\n\n- Before we can visualize PCA results, we need to actually identify PC axes!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform Principal Components Analysis\nlich_pc <- prcomp(x = varespec)\n\n# Summarize it to calculate '% variation explained' for each PC axis\nlich_pc_smry <- summary(lich_pc)\n\n# Check the structure of the summarized object\nstr(lich_pc_smry)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 6\n $ sdev      : num [1:24] 31.35 21.55 11.5 8.6 6.96 ...\n $ rotation  : num [1:44, 1:24] -0.01399 0.01566 -0.00646 -0.05168 0.00858 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:44] \"Callvulg\" \"Empenigr\" \"Rhodtome\" \"Vaccmyrt\" ...\n  .. ..$ : chr [1:24] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center    : Named num [1:44] 1.88 6.33 0.35 2.11 11.46 ...\n  ..- attr(*, \"names\")= chr [1:44] \"Callvulg\" \"Empenigr\" \"Rhodtome\" \"Vaccmyrt\" ...\n $ scale     : logi FALSE\n $ x         : num [1:24, 1:24] -10.8 -27.8 -25.7 -31.8 -19.6 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:24] \"18\" \"15\" \"24\" \"27\" ...\n  .. ..$ : chr [1:24] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ importance: num [1:3, 1:24] 31.352 0.538 0.538 21.548 0.254 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Standard deviation\" \"Proportion of Variance\" \"Cumulative Proportion\"\n  .. ..$ : chr [1:24] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"summary.prcomp\"\n```\n\n\n:::\n:::\n\n\n## [Principal Components Ordination]{.pink} {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# With that done, we can make a graph of that information!\nplot(x = lich_pc$x[,1], y = lich_pc$x[,2], pch = 20, \n     ## And do some fancy axis labels to get 'variation explained' in the plot\n     xlab = paste0(\"PC1 (\", (lich_pc_smry$importance[2, 1] * 100), \" % variation explained)\"), \n     ylab = paste0(\"PC2 (\", (lich_pc_smry$importance[2, 2] * 100), \" % variation explained)\"))\n```\n\n::: {.cell-output-display}\n![](slides_multivariate-stats_files/figure-revealjs/pca-ord-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## [Non-Metric Multidimensional Scaling]{.blue} {.smaller}\n\n- Just like PCA, we need to start by actually performing the scaling step\n\n\\\n\n- See here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform NMS ordination\nlich_mds <- vegan::metaMDS(comm = varespec, distance = \"bray\", k = 2, try = 50,\n                           autotransform = F, expand = F)\n```\n:::\n\n\n\\\n\n- Quick explanation of (some of) the arguments\n    - `distance` is your preferred metric for dissimilarity\n    - `k` is the number of axes to scale to (typically 2 for standard plotting)\n    - `try` is the number of starting data configurations (remember the helicopter analogy!)\n\n## [NMS Ordination]{.blue} {.smaller}\n\n- Fortunately, we can use a nice ordination function from the `supportR` package to make the graph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create NMS ordination\nsupportR::nms_ord(mod = lich_mds, groupcol = lichen_df$treatment)\n```\n\n::: {.cell-output-display}\n![](slides_multivariate-stats_files/figure-revealjs/nms-ord-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## [Thanks! Questions?]{.cream} {background-image=\"images_general/pic-question-mark-bfly.jpg\"}\n",
    "supporting": [
      "slides_multivariate-stats_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}